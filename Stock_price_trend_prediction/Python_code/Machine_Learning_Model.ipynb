{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW7h47qmpp_9",
        "outputId": "fb9fd54d-cad4-4550-d1a1-c9b7646a83f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 54.76%\n",
            "Precision: 54.12%\n",
            "Recall: 51.17%\n",
            "F1 Score: 52.61%\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "# Load data from CSV file\n",
        "file_path = '/content/drive/MyDrive/data_with_predictions.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Remove rows where any column has NaN values\n",
        "df = df.dropna()\n",
        "\n",
        "# Define features and target for Gaussian Naive Bayes\n",
        "X =df[['Open ','High ','Low','Shares Traded ', 'Turnover']]\n",
        "y = df['Prize Action']\n",
        "\n",
        "# Scale the features for Gaussian Naive Bayes\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split data into training and testing sets for Gaussian Naive Bayes\n",
        "X_train_nb, X_test_nb, y_train_nb, y_test_nb = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gaussian Naive Bayes classifier\n",
        "clf_nb = BernoulliNB()\n",
        "\n",
        "# Fit the Gaussian Naive Bayes model\n",
        "clf_nb.fit(X_train_nb, y_train_nb)\n",
        "y_pred_nb=clf_nb.predict(X_test_nb)\n",
        "# Predict probabilities on the test set\n",
        "y_pred_prob_nb = clf_nb.predict_proba(X_test_nb)[:, 1]  # Probability of the positive class\n",
        "accuracy = accuracy_score(y_test_nb, y_pred_nb)\n",
        "precision = precision_score(y_test_nb, y_pred_nb)\n",
        "recall = recall_score(y_test_nb, y_pred_nb)\n",
        "f1= f1_score(y_test_nb, y_pred_nb)\n",
        "# conf_matrix_rf = confusion_matrix(y_test_rf, y_pred_rf)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision * 100:.2f}%\")\n",
        "print(f\"Recall: {recall * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
        "# print(f\"Confusion Matrix:\\n{conf_matrix}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhZ2MU3usMwZ",
        "outputId": "925af44a-fd8d-4cd5-e6ae-25ea9473d8d5"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "# Define the directory path in Google Drive\n",
        "model_directory = '/content/drive/MyDrive/bert_model'\n",
        "\n",
        "# Load the BERT model and tokenizer\n",
        "model = BertForSequenceClassification.from_pretrained(model_directory)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define maximum token limit for BERT\n",
        "max_length = model.config.max_position_embeddings\n",
        "\n",
        "# Load data from CSV file\n",
        "csv_file_path = '/content/drive/MyDrive/dataset_small.csv'\n",
        "df = pd.read_csv(csv_file_path, encoding='ISO-8859-1')\n",
        "\n",
        "\n",
        "# Define a function to make predictions\n",
        "def predict(text):\n",
        "  try:\n",
        "      inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Check for presence of OOV words\n",
        "      # if tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) != text.split():  # Assuming tokenization splits on whitespace\n",
        "      #   print(f\"WARNING: Text contains OOV words. Using special tokens.\")\n",
        "      # print(len(inputs))\n",
        "      outputs = model(**inputs)\n",
        "      logits = outputs.logits\n",
        "      predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "      if predictions[0]==0:\n",
        "        return predictions[0],'Positive'\n",
        "      else:\n",
        "        return predictions[0],'Negative'\n",
        "  except Exception as e:\n",
        "        # Print the error value for debugging\n",
        "        print(f\"Error during prediction for text: {text}\\nError message: {e}\")\n",
        "        return None  # Or any desired default value\n"
      ],
      "metadata": {
        "id": "Eg6RfZ0f8e3A"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_test_nb))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S6cgPUDvoeZ",
        "outputId": "87f767ec-a2ea-49ea-b182-66b47084f8ae"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_lr = pd.DataFrame({\n",
        "    'GNB_Prob': y_pred_nb,\n",
        "    'Sentiment': X_test_nb[:, -1]\n",
        "})\n",
        "\n",
        "# Split data into training and testing sets for Logistic Regression\n",
        "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_lr, y_test_nb, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression classifier\n",
        "clf_lr = LogisticRegression()\n",
        "\n",
        "# Fit the Logistic Regression model\n",
        "clf_lr.fit(X_train_lr, y_train_lr)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_lr = clf_lr.predict(X_test_lr)\n",
        "\n",
        "# Evaluate the Logistic Regression model\n",
        "accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)\n",
        "precision_lr = precision_score(y_test_lr, y_pred_lr)\n",
        "recall_lr = recall_score(y_test_lr, y_pred_lr)\n",
        "f1_lr = f1_score(y_test_lr, y_pred_lr)\n",
        "conf_matrix_lr = confusion_matrix(y_test_lr, y_pred_lr)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Logistic Regression Classifier Metrics:\")\n",
        "print(f\"Accuracy: {accuracy_lr * 100:.2f}%\")\n",
        "print(f\"Precision: {precision_lr * 100:.2f}%\")\n",
        "print(f\"Recall: {recall_lr * 100:.2f}%\")\n",
        "print(f\"F1 Score: {f1_lr * 100:.2f}%\")\n",
        "print(f\"Confusion Matrix:\\n{conf_matrix_lr}\")"
      ],
      "metadata": {
        "id": "oAU_UfFBsqeY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cea0c54-cd0c-4556-ce8c-451014a7c98c"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Classifier Metrics:\n",
            "Accuracy: 55.44%\n",
            "Precision: 53.27%\n",
            "Recall: 51.73%\n",
            "F1 Score: 52.49%\n",
            "Confusion Matrix:\n",
            "[[337 236]\n",
            " [251 269]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "hAn1oHR3t6Lg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adani\n",
        "text=[\n",
        "    \"View: The Adani short sale puts investor trust in India in doubt\",\n",
        "    \"India’s Adani Group loses $48bn in stocks over fraud claims\",\n",
        "    \"Adani claims US investment firm’s fraud allegations are an ‘attack on India\",\n",
        "    \"Adani group loses $48 billion since January 25\"\n",
        "]\n",
        "data=np.array([[3300,3342.25,2712,8729,2.630224557]])\n",
        "data = scaler.fit_transform(data)\n",
        "sentiment=[]\n",
        "for i in text:\n",
        "  x,y=predict(i)\n",
        "  print(i,\":\",y)\n",
        "  sentiment.append(x)\n",
        "frequency_counter = Counter(sentiment)\n",
        "\n",
        "max_frequency = max(frequency_counter.values())\n",
        "\n",
        "final_sentiment = [number for number, frequency in frequency_counter.items() if frequency == max_frequency]\n",
        "\n",
        "y_pred_nb = clf_nb.predict(data)\n",
        "y_pred_nb = np.append(y_pred_nb, final_sentiment)\n",
        "fd=np.array([y_pred_nb])\n",
        "\n",
        "y_pred_lr = clf_lr.predict(fd)\n",
        "print(\"Price_action: \",y_pred_lr[0])"
      ],
      "metadata": {
        "id": "sQH6wdE4sDgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2ca0029-ce55-4202-db6f-028a5f389a7c"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View: The Adani short sale puts investor trust in India in doubt : Negative\n",
            "India’s Adani Group loses $48bn in stocks over fraud claims : Negative\n",
            "Adani claims US investment firm’s fraud allegations are an ‘attack on India : Negative\n",
            "Adani group loses $48 billion since January 25 : Negative\n",
            "Price_action:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PaytM\n",
        "text=[\n",
        "    \"RBI move on Paytm Payments Bank may be precursor to licence cancellation\",\n",
        "    \"Paytm may get NPCI nod for third-party application license but no relief for banking arm beyond March 15\"\n",
        "]\n",
        "data=np.array([[525,528,446.65,615676,2.998]])\n",
        "data = scaler.fit_transform(data)\n",
        "sentiment=[]\n",
        "for i in text:\n",
        "  x,y=predict(i)\n",
        "  print(i,\":\",y)\n",
        "  sentiment.append(x)\n",
        "frequency_counter = Counter(sentiment)\n",
        "\n",
        "max_frequency = max(frequency_counter.values())\n",
        "\n",
        "final_sentiment = [number for number, frequency in frequency_counter.items() if frequency == max_frequency]\n",
        "\n",
        "y_pred_nb = clf_nb.predict(data)\n",
        "y_pred_nb = np.append(y_pred_nb, final_sentiment)\n",
        "fd=np.array([y_pred_nb])\n",
        "\n",
        "y_pred_lr = clf_lr.predict(fd)\n",
        "print(\"Price_action: \",y_pred_lr[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKHnORUZ2Ops",
        "outputId": "b57d6956-fed6-4608-d0b7-6d6c66f0653a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RBI move on Paytm Payments Bank may be precursor to licence cancellation : Negative\n",
            "Paytm may get NPCI nod for third-party application license but no relief for banking arm beyond March 15 : Negative\n",
            "Price_action:  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6RxYBgLB_Cdz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
